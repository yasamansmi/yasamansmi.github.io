<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Transformers, Embeddings, and Neural Network Fundamentals</title>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1>Understanding Transformers, Embeddings, and Neural Network Fundamentals</h1>

    <h2>Transformers</h2>
    <p>Transformers revolutionized the machine learning field by moving away from procedural methods. Instead of step-by-step instructions, they establish flexible structures capable of learning through examples. This adaptability allows them to excel in tasks like natural language processing.</p>

    <h2>Linear Regression</h2>
    <p>Linear regression is one of the foundational algorithms in machine learning. It works by determining parameters (weights) that minimize the difference between the predicted and actual data. Mathematically:</p>
    <p>\\( f(x) = w_1x_1 + w_2x_2 + \\dots + w_nx_n \\)</p>

    <h2>Deep Learning and Neural Networks</h2>
    <ul>
        <li><strong>Inputs:</strong> Represented as arrays of real numbers (organized as tensors).</li>
        <li><strong>Final Layer:</strong> Outputs a probability distribution for predictions.</li>
    </ul>

    <h2>Weighted Sums in Neural Networks</h2>
    <p>Weighted sums form the basis of neural networks:</p>
    <p>\\[
    v = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}, \\quad
    w = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix}, \\quad
    v \\cdot w = v_1w_1 + v_2w_2 + \\dots + v_nw_n
    \\]</p>
    <p>Weights define the model, while the data vectors represent the input processed through the network.</p>

    <h2>Embedding Matrix</h2>
    <p>Embeddings transform words into numerical vectors, enabling models to understand relationships between words. An embedding matrix maps words to vector representations:</p>
    <ul>
        <li>Each word corresponds to a column in the matrix.</li>
        <li>Example: GPT-3 embeddings operate in a 12,288-dimensional space.</li>
    </ul>

    <h3>Embedding Arithmetic</h3>
    <p>Embeddings capture semantic relationships between words:</p>
    <ul>
        <li>\\( E(\\text{queen}) - E(\\text{woman}) \\approx E(\\text{king}) - E(\\text{man}) \\)</li>
        <li>\\( E(\\text{Hitler}) + E(\\text{Italy}) - E(\\text{Germany}) \\approx E(\\text{Mussolini}) \\)</li>
        <li>\\( E(\\text{sushi}) + E(\\text{Germany}) - E(\\text{Japan}) \\approx E(\\text{bratwurst}) \\)</li>
    </ul>

    <h2>Plurality Direction</h2>
    <p>The concept of plurality in embeddings can be expressed as:</p>
    <p>\\( \\text{plural} = E(\\text{cats}) - E(\\text{cat}) \\)</p>
    <p>This vector represents the plurality direction in the semantic space.</p>

    <h2>Context Size in Transformers</h2>
    <p>The context size defines how much data flows into the transformer at once. It depends on the number of vectors the transformer can process and incorporate into its computations.</p>

    <h2>Unembedding Matrix</h2>
    <p>The unembedding matrix maps outputs back into probabilities over the vocabulary. It has dimensions \\( n_{vocab} \\times d_{embed} \\).</p>

    <h2>Softmax Function</h2>
    <p>Softmax converts logits into probabilities, ensuring they sum to one:</p>
    <p>\\[
    \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{n=0}^{N-1} e^{x_n}}
    \\]</p>

    <h3>Softmax with Temperature</h3>
    <p>\\[
    \\text{softmax}\\left(\\frac{x_i}{T}\\right)
    \\]</p>
    <ul>
        <li><strong>Higher Temperature (T):</strong> Gives more weight to lower probability values.</li>
        <li><strong>Lower Temperature (T):</strong> Focuses more on higher probability values.</li>
    </ul>

    <h2>Inputs to Outputs</h2>
    <p>In neural networks, inputs are converted to logits, which are then transformed into probabilities through softmax.</p>

    <p>By breaking down these concepts, we can better appreciate the elegance of neural networks and their capabilities in solving real-world challenges.</p>
</body>
</html>
